name: Destroy Environment

on:
  workflow_dispatch:
    inputs:
      environment_name:
        description: "Name of the environment to destroy"
        required: true
        type: string

  workflow_call:
    inputs:
      environment_name:
        description: "Name of the environment to destroy"
        required: true
        type: string

permissions:
  id-token: write
  contents: read

env:
  STATE_BUCKET: ${{ vars.STATE_BUCKET }}
  AWS_REGION: ${{ vars.AWS_REGION }}
  LOCK_TABLE: ${{ vars.LOCK_TABLE }}
  ECR_REPO: ${{ vars.ECR_REPO }}

jobs:
  destroy:
    name: Destroy - ${{ inputs.environment_name }}
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Validate inputs
        run: |
          set -euo pipefail
          ENV_NAME="${{ inputs.environment_name }}"
          if [[ ! "$ENV_NAME" =~ ^[a-z0-9][a-z0-9-]{0,30}[a-z0-9]$ ]] && [[ ! "$ENV_NAME" =~ ^[a-z0-9]{2}$ ]]; then
            echo "::error::Invalid environment_name '${ENV_NAME}'. Must be 2-32 chars, lowercase alphanumeric and hyphens, cannot start/end with a hyphen."
            exit 1
          fi
          echo "Input validated: ${ENV_NAME}"

      - name: Checkout infrastructure code
        uses: actions/checkout@v4
        with:
          repository: AnerSarid/mini-idp
          ref: main

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install OpenTofu
        uses: opentofu/setup-opentofu@v1
        with:
          tofu_wrapper: false

      - name: Read environment metadata from S3
        id: metadata
        run: |
          set -euo pipefail
          METADATA_KEY="environments/${{ inputs.environment_name }}/metadata.json"

          if ! aws s3 cp "s3://${STATE_BUCKET}/${METADATA_KEY}" metadata.json; then
            echo "::error::Metadata not found for '${{ inputs.environment_name }}'."
            exit 1
          fi

          TEMPLATE=$(jq -r '.template' metadata.json)
          if [[ -z "$TEMPLATE" || "$TEMPLATE" == "null" ]]; then
            echo "::error::Could not determine template from metadata."
            exit 1
          fi

          echo "template=${TEMPLATE}" >> "$GITHUB_OUTPUT"

      - name: Download tfvars from S3
        id: tfvars
        run: |
          set -euo pipefail
          TFVARS_FILE="infrastructure/environments/${{ inputs.environment_name }}.tfvars.json"
          mkdir -p "$(dirname "$TFVARS_FILE")"

          # The provision workflow uploads the exact tfvars used to create the
          # environment. Download them â€” no reconstruction needed.
          if aws s3 cp "s3://${STATE_BUCKET}/environments/${{ inputs.environment_name }}/tfvars.json" \
               "$TFVARS_FILE" 2>/dev/null; then
            echo "Downloaded tfvars from S3."
          else
            # Fallback for environments provisioned before tfvars were stored.
            echo "::warning::tfvars.json not found in S3. Building minimal tfvars from metadata."
            TEMPLATE="${{ steps.metadata.outputs.template }}"

            jq '{
              environment_name: .name,
              owner: (.owner // "unknown"),
              ttl: (.ttl // "7d"),
              created_at: (.created_at // "2000-01-01T00:00:00Z"),
              expires_at: (.expires_at // "2000-01-01T00:00:00Z")
            }' metadata.json > "$TFVARS_FILE"

            if [[ "$TEMPLATE" == "scheduled-worker" ]]; then
              jq '. + {schedule_expression: "rate(1 hour)", s3_bucket_arn: ""}' \
                "$TFVARS_FILE" > "$TFVARS_FILE.tmp" && mv "$TFVARS_FILE.tmp" "$TFVARS_FILE"
            fi
          fi

          echo "tfvars_file=${TFVARS_FILE}" >> "$GITHUB_OUTPUT"

      - name: Copy shared base files into template
        run: cp infrastructure/templates/_base/*.tf "infrastructure/templates/${{ steps.metadata.outputs.template }}/"

      - name: Tofu init
        working-directory: infrastructure/templates/${{ steps.metadata.outputs.template }}
        run: |
          tofu init -input=false \
            -backend-config="bucket=${STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${LOCK_TABLE}" \
            -backend-config="key=environments/${{ inputs.environment_name }}/terraform.tfstate"

      - name: Tofu destroy
        working-directory: infrastructure/templates/${{ steps.metadata.outputs.template }}
        run: |
          tofu destroy \
            -var-file="${{ github.workspace }}/${{ steps.tfvars.outputs.tfvars_file }}" \
            -auto-approve \
            -input=false \
            -lock-timeout=5m

      - name: Clean up S3 artifacts
        run: |
          set -euo pipefail
          aws s3 rm "s3://${STATE_BUCKET}/environments/${{ inputs.environment_name }}/" --recursive

      - name: Clean up DynamoDB state digest
        continue-on-error: true
        run: |
          set -euo pipefail
          STATE_KEY="environments/${{ inputs.environment_name }}/terraform.tfstate"
          aws dynamodb delete-item \
            --table-name "${LOCK_TABLE}" \
            --key "{\"LockID\":{\"S\":\"${STATE_BUCKET}/${STATE_KEY}-md5\"}}" \
            2>/dev/null || true
          echo "DynamoDB digest entry cleaned up."

      - name: Clean up ECR images
        continue-on-error: true
        run: |
          set -euo pipefail

          ENV_NAME="${{ inputs.environment_name }}"

          echo "Cleaning up ECR images tagged with '${ENV_NAME}-*'..."

          # List images matching this environment's tag prefix
          IMAGE_IDS=$(aws ecr list-images \
            --repository-name "${ECR_REPO}" \
            --filter tagStatus=TAGGED \
            --query "imageIds[?starts_with(imageTag, '${ENV_NAME}-')]" \
            --output json 2>/dev/null || echo "[]")

          if [[ "$IMAGE_IDS" == "[]" || -z "$IMAGE_IDS" ]]; then
            echo "No ECR images found for environment '${ENV_NAME}'."
            exit 0
          fi

          IMAGE_COUNT=$(echo "$IMAGE_IDS" | jq 'length')
          echo "Found ${IMAGE_COUNT} image(s) to delete."

          aws ecr batch-delete-image \
            --repository-name "${ECR_REPO}" \
            --image-ids "$IMAGE_IDS"

          echo "ECR images cleaned up successfully."
